# Chapter 1: Transformer

Prerequisite:
1. Luis Serrano, A friendly introduction to Recurrent Neural Networks <br>
   https://www.youtube.com/watch?v=UNmqTiOnRfg

Reading Materials:
2. Attention Blog post <br>
   https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/<br>
   seq to seq and Attention: https://docs.pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html<br>
3. Attention is all you need: https://arxiv.org/pdf/1706.03762 <br>
ï¼ˆvideo analysis: https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.337.search-card.all.click&vd_source=75e16b30403690b6ad4ccdb9c2dbde46)
4. Illustrated Transformer Blog post <br>
    https://jalammar.github.io/illustrated-transformer/

Practical Implementation:
1. Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out. <br>
 https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6067s
 
 Solution to Deep Learning Curriculum Chapter 1:
 
